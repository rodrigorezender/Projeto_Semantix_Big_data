{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Semantix - Dados Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-24 19:27 /user/hive/warehouse/dados_covid\r\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-24 13:45 /user/hive/warehouse/projeto_semantix.db\r\n"
     ]
    }
   ],
   "source": [
    "# Listar arquivos no hive/warehouse`\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Leitura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codregiaosaude: integer (nullable = true)\n",
      " |-- nomeregiaosaude: string (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      " |-- semanaepi: integer (nullable = true)\n",
      " |-- populacaotcu2019: integer (nullable = true)\n",
      " |-- casosacumulado: integer (nullable = true)\n",
      " |-- casosnovos: integer (nullable = true)\n",
      " |-- obitosacumulado: integer (nullable = true)\n",
      " |-- obitosnovos: integer (nullable = true)\n",
      " |-- recuperadosnovos: integer (nullable = true)\n",
      " |-- emacompanhamentonovos: integer (nullable = true)\n",
      " |-- interior_metropolitana: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ler dados da tabela Hive\n",
    "\n",
    "tabela_dados_covid = spark.read.table(\"dados_covid\")\n",
    "tabela_dados_covid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries\n",
    "\n",
    "total_casos_recuperados = spark.sql(\"select MAX (recuperadosnovos) as Casos_Recuperados from dados_covid\")\n",
    "casos_em_acompanhamento = spark.sql(\"select LAST (emacompanhamentonovos) as Em_Acompanhamento from dados_covid WHERE emacompanhamentonovos IS NOT NULL\")\n",
    "\n",
    "### Fórmulas ###\n",
    "\n",
    "\n",
    "###Calculo incidencia (casos confirmados * 1.000.000) / população.\n",
    "###Calculo letalidade (mortes totais/casos totais)\n",
    "###Calculo mortalidade (mortes totais/população)\n",
    "\n",
    "total_casos_acumulados = spark.sql(\"select MAX (casosacumulado) as Acumulado from dados_covid \")\n",
    "casos_novos = spark.sql(\"select MAX (casosnovos) as Casos_novos from dados_covid where data = ('2021-07-06')\")\n",
    "incidencia = spark.sql(\"SELECT ROUND(((MAX(casosacumulado) / MAX(populacaotcu2019))*100000),1) as incidencia from dados_covid where data = ('2021-07-06')\")\n",
    "\n",
    "total_obitos_acumulados = spark.sql(\"select MAX (obitosacumulado) as Obito_acumulado from dados_covid \")\n",
    "obitos_novos = spark.sql(\"select MAX (obitosnovos) as Obitos_novos from dados_covid where data = ('2021-07-06')\")\n",
    "letalidade = spark.sql(\"SELECT ROUND(((MAX(obitosacumulado) / MAX(casosacumulado))*100),1) as letalidade from dados_covid\")\n",
    "mortalidade = spark.sql(\"SELECT ROUND(((MAX(obitosacumulado) / MAX(populacaotcu2019))*100000),1) as mortalidade from dados_covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Casos_Recuperados|\n",
      "+-----------------+\n",
      "|         17262646|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|Em_Acompanhamento|\n",
      "+-----------------+\n",
      "|          1065477|\n",
      "+-----------------+\n",
      "\n",
      "+---------+\n",
      "|Acumulado|\n",
      "+---------+\n",
      "| 18855015|\n",
      "+---------+\n",
      "\n",
      "+-----------+\n",
      "|Casos_novos|\n",
      "+-----------+\n",
      "|      62504|\n",
      "+-----------+\n",
      "\n",
      "+----------+\n",
      "|incidencia|\n",
      "+----------+\n",
      "|    8972.3|\n",
      "+----------+\n",
      "\n",
      "+---------------+\n",
      "|Obito_acumulado|\n",
      "+---------------+\n",
      "|         526892|\n",
      "+---------------+\n",
      "\n",
      "+------------+\n",
      "|Obitos_novos|\n",
      "+------------+\n",
      "|        1780|\n",
      "+------------+\n",
      "\n",
      "+----------+\n",
      "|letalidade|\n",
      "+----------+\n",
      "|       2.8|\n",
      "+----------+\n",
      "\n",
      "+-----------+\n",
      "|mortalidade|\n",
      "+-----------+\n",
      "|      250.7|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_casos_recuperados.show()\n",
    "casos_em_acompanhamento.show()\n",
    "\n",
    "total_casos_acumulados.show()\n",
    "casos_novos.show()\n",
    "incidencia.show()\n",
    "\n",
    "total_obitos_acumulados.show()\n",
    "obitos_novos.show()\n",
    "letalidade.show()\n",
    "mortalidade.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View CASOS RECUPERADOS\n",
    "spark.sql(\"CREATE OR REPLACE VIEW Total_Casos_Recuperados AS\\\n",
    "        SELECT MAX (recuperadosnovos) AS Casos_Recuperados, \\\n",
    "        MAX (emacompanhamentonovos) AS Em_Acompanhamento \\\n",
    "        FROM dados_covid \\\n",
    "        WHERE data = ('2021-07-06')\")\n",
    "\n",
    "# View CASOS CONFIRMADOS\n",
    "spark.sql(\"CREATE OR REPLACE VIEW Casos_Confirmados AS \\\n",
    "        SELECT MAX (casosacumulado) AS Acumulado, \\\n",
    "        ROUND(((MAX(casosacumulado) / MAX(populacaotcu2019))*100000),1) AS Incidencia, \\\n",
    "        MAX (casosnovos) AS Casos_novos \\\n",
    "        FROM dados_covid \\\n",
    "        WHERE data = ('2021-07-06')\")\n",
    "\n",
    "# View OBITOS CONFIRMADOS\n",
    "spark.sql(\"CREATE OR REPLACE VIEW Obitos_Confirmados AS \\\n",
    "        SELECT MAX (obitosacumulado) AS Obitos_acumulados, \\\n",
    "        MAX (casosnovos) AS Casos_novos, \\\n",
    "        ROUND(((MAX(obitosacumulado) / MAX(casosacumulado))*100),1) AS Letalidade, \\\n",
    "        ROUND(((MAX(obitosacumulado) / MAX(populacaotcu2019))*100000),1) AS Mortalidade \\\n",
    "        FROM dados_covid \\\n",
    "        WHERE data = ('2021-07-06')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|Casos_Recuperados|Em_Acompanhamento|\n",
      "+-----------------+-----------------+\n",
      "|         17262646|          1065477|\n",
      "+-----------------+-----------------+\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|Acumulado|Incidencia|Casos_novos|\n",
      "+---------+----------+-----------+\n",
      "| 18855015|    8972.3|      62504|\n",
      "+---------+----------+-----------+\n",
      "\n",
      "+-----------------+-----------+----------+-----------+\n",
      "|Obitos_acumulados|Casos_novos|Letalidade|Mortalidade|\n",
      "+-----------------+-----------+----------+-----------+\n",
      "|           526892|      62504|       2.8|      250.7|\n",
      "+-----------------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view1 = spark.sql(\"SELECT * FROM Total_Casos_Recuperados\")\n",
    "view2 = spark.sql(\"SELECT * FROM Casos_Confirmados\")\n",
    "view3 = spark.sql(\"SELECT * FROM Obitos_Confirmados\")\n",
    "\n",
    "view1.show()\n",
    "view2.show()\n",
    "view3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar view 1 como tabela Hive\n",
    "\n",
    "view1.write.saveAsTable(\"view1\")\n",
    "\n",
    "# Salvar view 2 como formato parquet e compressão snappy\n",
    "\n",
    "view2.write.option(\"compression\", \"snappy\").parquet(\"/user/hive/warehouse/view2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-24 23:25 /user/hive/warehouse/view1/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        679 2022-04-24 23:25 /user/hive/warehouse/view1/part-00000-6e6b2967-e529-427d-9850-332238bf7e37-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# Verificar arquivo salvo em /user/hive/warehouse\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/view1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-24 23:29 /user/hive/warehouse/view2/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        894 2022-04-24 23:29 /user/hive/warehouse/view2/part-00000-391af4a2-357c-4237-8a86-f4317b93ec4d-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# Verificar arquivo salvo em /user/hive/warehouse\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+----------+-----------+--------------------+\n",
      "|Obitos_acumulados|Casos_novos|Letalidade|Mortalidade|               value|\n",
      "+-----------------+-----------+----------+-----------+--------------------+\n",
      "|           526892|      62504|       2.8|      250.7|[526892, 62504, 2...|\n",
      "+-----------------+-----------+----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salvar view 3 em um tópico no Kafka\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "view3_convertido = view3.withColumn(\n",
    "            \"value\", struct(\n",
    "            col(\"Obitos_acumulados\"), \\\n",
    "            col(\"Casos_novos\"), \\\n",
    "            col(\"Letalidade\"), \\\n",
    "            col(\"Mortalidade\")))\n",
    "view3_convertido = view3_convertido.withColumn(\"value\",col(\"value\").cast(\"string\"))\n",
    "\n",
    "view3_convertido.show()\n",
    "\n",
    "view3_convertido.write.format(\"kafka\")\n",
    "        \\.option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "        \\.option(\"topic\",\"view3\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar visualização com os dados enviados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Região</th>\n",
       "      <th>Casos</th>\n",
       "      <th>Óbitos</th>\n",
       "      <th>Incidência/mil hab.</th>\n",
       "      <th>Mortalidade/mil hab.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>4455737</td>\n",
       "      <td>107824</td>\n",
       "      <td>7,807.3</td>\n",
       "      <td>188.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sul</td>\n",
       "      <td>3611041</td>\n",
       "      <td>80705</td>\n",
       "      <td>12,046.4</td>\n",
       "      <td>269.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sudeste</td>\n",
       "      <td>7138803</td>\n",
       "      <td>245311</td>\n",
       "      <td>8,078.2</td>\n",
       "      <td>277.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Centro-Oeste</td>\n",
       "      <td>1916619</td>\n",
       "      <td>49207</td>\n",
       "      <td>11,760.5</td>\n",
       "      <td>301.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brasil</td>\n",
       "      <td>18855015</td>\n",
       "      <td>526892</td>\n",
       "      <td>8,972.3</td>\n",
       "      <td>250.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Norte</td>\n",
       "      <td>1732862</td>\n",
       "      <td>43845</td>\n",
       "      <td>9,401.9</td>\n",
       "      <td>237.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Região     Casos  Óbitos Incidência/mil hab. Mortalidade/mil hab.\n",
       "0      Nordeste   4455737  107824             7,807.3                188.9\n",
       "1           Sul   3611041   80705            12,046.4                269.2\n",
       "2       Sudeste   7138803  245311             8,078.2                277.6\n",
       "3  Centro-Oeste   1916619   49207            11,760.5                301.9\n",
       "4        Brasil  18855015  526892             8,972.3                250.7\n",
       "5         Norte   1732862   43845             9,401.9                237.9"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "view_spark = tabela_dados_covid.select(\n",
    "    'regiao',\\\n",
    "    'populacaotcu2019',\\\n",
    "    'obitosacumulado',\\\n",
    "    'casosacumulado',\\\n",
    "    'estado').where(tabela_dados_covid.regiao != 'regiao')\n",
    "\n",
    "view_spark = view_spark.groupBy('estado', 'regiao')\n",
    "view_spark = view_spark.max('casosacumulado','obitosacumulado','populacaotcu2019')\n",
    "\n",
    "view_spark = view_spark.groupBy('regiao')\n",
    "view_spark = view_spark.sum('max(casosacumulado)','max(obitosacumulado)','max(populacaotcu2019)')\n",
    "view_spark = view_spark.withColumn('mult', lit(100000))\n",
    "\n",
    "view_spark = view_spark.withColumn('Incidência/mil hab.', \\\n",
    "        format_number(((col('sum(max(casosacumulado))') * col('mult'))/col('sum(max(populacaotcu2019))')),1))\n",
    "view_spark = view_spark.withColumn('Mortalidade/mil hab.', \\\n",
    "        format_number(((col('sum(max(obitosacumulado))')/col('sum(max(populacaotcu2019))'))) * col('mult'),1))\n",
    "\n",
    "view_spark = view_spark.drop('mult','sum(max(populacaotcu2019))')\n",
    "view_spark = view_spark.withColumnRenamed('sum(max(casosacumulado))', 'Casos').\\\n",
    "        withColumnRenamed('sum(max(obitosacumulado))', 'Óbitos').\\\n",
    "        withColumnRenamed('regiao', 'Região')\n",
    "\n",
    "view_spark.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar view3 em um tópico no Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-25 02:00 /user/hive/warehouse/elastic_search/view3_elastic.csv/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup         76 2022-04-25 02:00 /user/hive/warehouse/elastic_search/view3_elastic.csv/part-00000-4ceed0e5-d0b7-4ee9-ab11-b377f0a97bd1-c000.csv\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obitos_acumulados</th>\n",
       "      <th>Casos_novos</th>\n",
       "      <th>Letalidade</th>\n",
       "      <th>Mortalidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>526892</td>\n",
       "      <td>62504</td>\n",
       "      <td>2.8</td>\n",
       "      <td>250.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Obitos_acumulados  Casos_novos  Letalidade  Mortalidade\n",
       "0             526892        62504         2.8        250.7"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view3_el = view3.write.format('csv')\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"/user/hive/warehouse/elastic_search/view3_elastic.csv\")\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/elastic_search/view3_elastic.csv\n",
    "\n",
    "spark.read.csv('/user/hive/warehouse/elastic_search/view3_elastic.csv', inferSchema = True, header = True).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
